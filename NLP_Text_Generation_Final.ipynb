{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Text Generation"
      ],
      "metadata": {
        "id": "WmyMMKuS8XUs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to Get Started\n",
        "\n",
        "---\n",
        "\n",
        "## Before Run\n",
        "1. Upload the trained model weights `scifi_best_model.hdf5`.\n",
        "2. Make sure all the file directories are set as the right path.\n",
        "3. You might want to delete `fit()` in `LSTM - Final Model` category, if you want to use the model uploaded from the step 1.\n",
        "4. Run the code.\n",
        "\n",
        "## Generating Text\n",
        "1. In `Text Generation` category, you can chage `given_word` and the result will be shown as `generated_text`\n",
        "2. The example for `given_word` are as follows:\n",
        "  * 'Time travel exists only in the'\n",
        "  * 'I am not'\n",
        "  * 'The car'\n",
        "  \n",
        "  However, you can type your own `given_word`.\n"
      ],
      "metadata": {
        "id": "7W4tv0Al2_UP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1Nc8OI8EbFr"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nN22TzZwbYUj"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import string\n",
        "\n",
        "import gensim\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Activation, Dropout\n",
        "from keras.callbacks import LambdaCallback\n",
        "\n",
        "import re\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVlAjRZdbYUk",
        "outputId": "dcda3e49-65ca-4bce-e9bc-ef04793998ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ywyJl41EbFs"
      },
      "outputs": [],
      "source": [
        "def get_txtfile(filename):\n",
        "  file = open(filename, \"r\")\n",
        "  txt = file.read()\n",
        "  file.close()\n",
        "  \n",
        "  return txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3oFz1DGhEbFs"
      },
      "outputs": [],
      "source": [
        "text = get_txtfile(\"/content/drive/My Drive/Colab Notebooks/PGT/CSC8637_DeepLearning/Coursework/Dataset/SciFi/internet_archive_scifi_v3.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YunXOo7BEbFs"
      },
      "source": [
        "Show the loaded data which is a raw text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "emlLo51jEbFs",
        "outputId": "446320c5-8b92-4557-a11c-6b99c0385648"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'MARCH # All Stories New and Complete Publisher Editor IF is published bi-monthly by Quinn Publishing Company, Inc., Kingston, New York. Volume #, No. #. Copyright # by Quinn Publishing Company, Inc. Application for Entry\\' as Second Class matter at Post Office, Buffalo, New York, pending. Subscription # for # issues in U.S. and Possessions: Canada # for # issues; elsewhere #. Aiiow four weeks for change of address. All stories appearing in this magazine are fiction. Any similarity to actual persons is coincidental. #c a fcopy. Printed ia U.S. A. A chat with the editor  i #  science fiction magazine called IF. The title was selected after much thought because of its brevity and on the theory it is indicative of the field and will be easy to remember. The tentative title that just morning and couldn\\'t remember it until we\\'d had a cup of coffee, it was summarily discarded. A great deal of thought and effort lias gone into the formation of this magazine. We have had the aid of several very talented and generous people, for which we are most grateful. Much is due them for their warmhearted assistance. And now that the bulk of the formative work is done, we will try to maintain IF as one of the finest books on the market.  t a great public demand for our magazine. In short, why will you buy IF? We cannot, in honesty, say we will publish at all times the best science fiction in the field. That would not be true. But we will have access to the best stories, and we will get our fair share of works from the best writers. We definitely will not talk \"adult\" or \"juvenile\" relative to our content as we feel such terms are misleading. We would rather think at all times in the terms of \"story\". Some of the greatest escapist literature ever written, Treasure Island for instance, could be put into either category or both. And if Edgar Rice Burroughs is juvenile, then so are we, because the late master has given us some memorable thrills. Frankly, we don\\'t think you\\'ll buy IF because you feel we print better yams than any other mag. You will buy it, we hope, because you like its personality. Every magazine, we feel, does have a definite personality of its own. This personality is usually a reflection of the editors, their way of thinking, their appreciation of tKe market, their interpretation of what you will like best in stories and artwork. We have tried to make IF different from any other science fiction magazine on the stands while still building it along the lines of what every science fiction mag must be. Aside from the letter columns and the editorial, which are departments of field-wide use, we have not copied any feature of any other magazine. We will not, for instance, review fanzines, because we feel that is being most ably done by other mags. Nor will we, as a general practice, review books because that appears to us to be overdone. a personality of our own and hope thereby to establish an affinity with a large number of readers who will remember IF when they buy a science fiction mag as one they like and wish to continue reading. At all times we will hew to the story-line and will exhort with our writers to do the same. As an example, when Howard Browne phoned to talk over the plot for his lead novel in this issue, he described what ivas without doubt a staggering premise, a really startling concept. \"But,\" he mourned, S T suppose I\\'ll have to bend it around to give them the good old conventional ending.\" We told Howard, \"Not for IF, chum. Remember the old creed we live by. A writer may cheat on his wife, but he is ever true to the story-line. He may haul his infant son around by one leg. but he carries a good story-idea like a holy relic. If there is only one logical ending for Twelve Times ZerG, that\\'s the ending we want.\" Therefore, we do not feel the majority of readers necessarily want a happy ending regardless of all else. Not when it is incompatable with the aura of realism created by the writer. A check-list of fiction masterpieces certainly bears this out. The furor created by a little piece called Sorry , Wrong Number would certainly not have been forthcoming had the bedridden lady been rescued in the last paragraph. Romeo and Juliet would have beep nothing more than the smooth effort of the world\\'s greatest writer if Romeo had gotten there in time. Yet, in modern fiction, he gets there in time with such amazing regularity one feels he has memorized at least  a dozen time-tables. The result has been unnumbered carloads of mediocre fiction. Also -- though we don\\'t wish to underscore the point too heavily -- what could more surely have smothered the greatness of Wuthering Heights than a happy ending?  that IF will be a magazine given over to tragedy. W e will only insist that our writers create scenes and climaxes that fix the story rather than cater to that old \"debil\" formula. And in so doing we have an entirely selfish motive. This: As the years go by, we want to look back with personal pride upon an e'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "text[:5000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FGcUXSsrwGm",
        "outputId": "62c77c43-d323-497c-c54f-525f96201752"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text length:  149326361\n"
          ]
        }
      ],
      "source": [
        "print('text length: ', len(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srtRFH84dIob"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "No_RlkVyhVoc"
      },
      "outputs": [],
      "source": [
        "# sample_text = \"\"\"Mr. Smith bought cheapsite.com for 1.5 million dollars, i.e. he paid a lot for it. Did he mind? Adam Jones Jr. thinks he didn't. In any case, this isn't true... Well, with a probability of .9 it isn't.\"\"\"\n",
        "\n",
        "def standardize_text(raw_text):\n",
        "  # Split 'raw text' into 'sentences'\n",
        "  sentences = re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', raw_text)\n",
        "  # Tokenisation\n",
        "  tokens = [text_to_word_sequence(sentence) for sentence in sentences]\n",
        "  \n",
        "  return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nkn5w4ckffV"
      },
      "outputs": [],
      "source": [
        "tokenized_sentences = standardize_text(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isi37UMkctjz"
      },
      "source": [
        "# Word2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukdn_tEum0GW",
        "outputId": "bc1ebd70-2f71-4e02-af8f-ddc0cb66efbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.base_any2vec:consider setting layer size to a multiple of 4 for greater performance\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result embedding shape: (257011, 50)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-7cae22f2f2df>:8: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
            "  pretrained_weights = word_model.wv.syn0\n"
          ]
        }
      ],
      "source": [
        "word_model = gensim.models.Word2Vec(tokenized_sentences,\n",
        "                                    size=50, \n",
        "                                    min_count=1, \n",
        "                                    window=5,\n",
        "                                    iter=3\n",
        "                                    )\n",
        "\n",
        "pretrained_weights = word_model.wv.syn0\n",
        "vocab_size, embedding_size = pretrained_weights.shape\n",
        "print('Result embedding shape:', pretrained_weights.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7k_VqonZor-9",
        "outputId": "12459d9b-2893-4c42-fb65-e1291d98e547"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('prince', 0.80077064037323),\n",
              " ('queen', 0.7969905734062195),\n",
              " ('murray', 0.7541114687919617),\n",
              " ('founder', 0.7420518398284912),\n",
              " ('dictator', 0.7419301867485046),\n",
              " ('emperor', 0.7377235889434814),\n",
              " ('alexander', 0.716799259185791),\n",
              " ('elder', 0.716194212436676),\n",
              " ('leinster', 0.7158246636390686),\n",
              " ('youngest', 0.7124871015548706)]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# Check similar words\n",
        "word_sample = \"king\"\n",
        "word_model.wv.most_similar(word_sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5Eam14ppUqg"
      },
      "source": [
        "# Prepare Train Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64st7QbXpTBR",
        "outputId": "60712e39-6cab-458b-bcba-02a02068452a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max Length: 404\n"
          ]
        }
      ],
      "source": [
        "maxlen = max([len(v) for v in tokenized_sentences])\n",
        "print(\"Max Length:\", maxlen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWbiNEZGp4jm"
      },
      "outputs": [],
      "source": [
        "def word2idx(word):\n",
        "  return word_model.wv.vocab[word].index\n",
        "  \n",
        "def idx2word(idx):\n",
        "  return word_model.wv.index2word[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDMY-5oIplDl"
      },
      "outputs": [],
      "source": [
        "train_x = np.zeros([len(tokenized_sentences), maxlen], dtype=np.int32)\n",
        "train_y = np.zeros([len(tokenized_sentences)], dtype=np.int32)\n",
        "\n",
        "for i, sentence in enumerate(tokenized_sentences):\n",
        "  for t, word in enumerate(sentence[:-1]):\n",
        "    train_x[i, t] = word2idx(word)\n",
        "  train_y[i] = word2idx(sentence[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqpPgn68qVWm"
      },
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zdoemqd_Ktt"
      },
      "source": [
        "## Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMD4qOh__NWi"
      },
      "outputs": [],
      "source": [
        "# lstm_units = [64, 128, 256]\n",
        "# dropout_rates = [0.2, 0.3, 0.4]\n",
        "# learning_rates = [0.01, 0.001, 0.0001]\n",
        "\n",
        "# for units in lstm_units:\n",
        "#     for rate in dropout_rates:\n",
        "#         for lr in learning_rates:\n",
        "#             # Define checkpoint to save best model\n",
        "            \n",
        "#             # Train model\n",
        "#             model = Sequential()\n",
        "#             model.add(Embedding(\n",
        "#                 input_dim=vocab_size, \n",
        "#                 output_dim=embedding_size, \n",
        "#                 weights=[pretrained_weights]\n",
        "#                 )\n",
        "#             )\n",
        "#             model.add(LSTM(units))\n",
        "#             model.add(Dropout(rate))\n",
        "#             model.add(Dense(\n",
        "#                 units=vocab_size, \n",
        "#                 activation=\"softmax\"\n",
        "#                 )\n",
        "#             )\n",
        "#             model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=lr), loss='sparse_categorical_crossentropy')\n",
        "#             print(\"--------------------\")\n",
        "#             print('lstm_units:',units)\n",
        "#             print('dropout_rates:',rate)\n",
        "#             print('learning_rates:',lr)\n",
        "\n",
        "#             model.fit(train_x, train_y,\n",
        "#                     batch_size=128,\n",
        "#                     epochs=3\n",
        "#                     )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWEczNKPMsAF"
      },
      "source": [
        "## Final Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrvAo2Bkxad4",
        "outputId": "6fcb8599-cd37-4ab9-f3b6-05baf7e05307"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab_size: 257011\n"
          ]
        }
      ],
      "source": [
        "print(\"vocab_size:\", vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOxXmucyqWtl",
        "outputId": "a8d6682d-7727-4ffb-b58d-356d715335cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 50)          12850550  \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, None, 128)         91648     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, None, 128)         0         \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 128)               131584    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 257011)            33154419  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 46,228,201\n",
            "Trainable params: 46,228,201\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from keras.layers import Input, LSTM, RepeatVector, TimeDistributed\n",
        "from keras.models import Model\n",
        "\n",
        "num_units = 128\n",
        "dropout_rate = 0.3\n",
        "lr = 0.01\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(\n",
        "    input_dim=vocab_size, \n",
        "    output_dim=embedding_size, \n",
        "    weights=[pretrained_weights])\n",
        ")\n",
        "model.add(LSTM(units=num_units, return_sequences=True))\n",
        "model.add(Dropout(dropout_rate))\n",
        "model.add(LSTM(units=num_units))\n",
        "model.add(Dropout(dropout_rate))\n",
        "model.add(Dense(\n",
        "    units=vocab_size, \n",
        "    activation=\"softmax\"\n",
        "    ))\n",
        "\n",
        "checkpoint_filepath = 'scifi_best_model.hdf5'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='loss',\n",
        "    mode='min',\n",
        "    save_best_only=True)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=lr), loss='sparse_categorical_crossentropy')\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3ea1ad5-7f43-4b5c-f000-3d0c66cd2d49",
        "id": "neKsWSxX2WHV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5452\n",
            "Epoch 2/100\n",
            "706/706 [==============================] - 93s 132ms/step - loss: 8.5428\n",
            "Epoch 3/100\n",
            "706/706 [==============================] - 92s 131ms/step - loss: 8.5404\n",
            "Epoch 4/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5386\n",
            "Epoch 5/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5364\n",
            "Epoch 6/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5350\n",
            "Epoch 7/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5331\n",
            "Epoch 8/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5321\n",
            "Epoch 9/100\n",
            "706/706 [==============================] - 92s 131ms/step - loss: 8.5308\n",
            "Epoch 10/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5295\n",
            "Epoch 11/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5286\n",
            "Epoch 12/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5280\n",
            "Epoch 13/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5269\n",
            "Epoch 14/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5260\n",
            "Epoch 15/100\n",
            "706/706 [==============================] - 93s 131ms/step - loss: 8.5254\n",
            "Epoch 16/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5245\n",
            "Epoch 17/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5238\n",
            "Epoch 18/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5232\n",
            "Epoch 19/100\n",
            "706/706 [==============================] - 91s 130ms/step - loss: 8.5226\n",
            "Epoch 20/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5222\n",
            "Epoch 21/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5425\n",
            "Epoch 22/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5262\n",
            "Epoch 23/100\n",
            "706/706 [==============================] - 91s 129ms/step - loss: 8.5240\n",
            "Epoch 24/100\n",
            "706/706 [==============================] - 91s 129ms/step - loss: 8.5234\n",
            "Epoch 25/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5225\n",
            "Epoch 26/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5219\n",
            "Epoch 27/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5216\n",
            "Epoch 28/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5209\n",
            "Epoch 29/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5207\n",
            "Epoch 30/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5202\n",
            "Epoch 31/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5222\n",
            "Epoch 32/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5203\n",
            "Epoch 33/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5204\n",
            "Epoch 34/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5215\n",
            "Epoch 35/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5198\n",
            "Epoch 36/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5195\n",
            "Epoch 37/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5195\n",
            "Epoch 38/100\n",
            "706/706 [==============================] - 92s 131ms/step - loss: 8.5189\n",
            "Epoch 39/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5189\n",
            "Epoch 40/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5185\n",
            "Epoch 41/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5187\n",
            "Epoch 42/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5186\n",
            "Epoch 43/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5182\n",
            "Epoch 44/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5236\n",
            "Epoch 45/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5187\n",
            "Epoch 46/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5182\n",
            "Epoch 47/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5183\n",
            "Epoch 48/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5180\n",
            "Epoch 49/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5183\n",
            "Epoch 50/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5179\n",
            "Epoch 51/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5181\n",
            "Epoch 52/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5180\n",
            "Epoch 53/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5179\n",
            "Epoch 54/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5951\n",
            "Epoch 55/100\n",
            "706/706 [==============================] - 92s 131ms/step - loss: 8.5247\n",
            "Epoch 56/100\n",
            "706/706 [==============================] - 92s 131ms/step - loss: 8.5260\n",
            "Epoch 57/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5179\n",
            "Epoch 58/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5180\n",
            "Epoch 59/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5248\n",
            "Epoch 60/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5179\n",
            "Epoch 61/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5240\n",
            "Epoch 62/100\n",
            "706/706 [==============================] - 92s 131ms/step - loss: 8.5205\n",
            "Epoch 63/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5192\n",
            "Epoch 64/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5196\n",
            "Epoch 65/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5194\n",
            "Epoch 66/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5196\n",
            "Epoch 67/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5194\n",
            "Epoch 68/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5193\n",
            "Epoch 69/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5195\n",
            "Epoch 70/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5196\n",
            "Epoch 71/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5194\n",
            "Epoch 72/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5191\n",
            "Epoch 73/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5199\n",
            "Epoch 74/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5197\n",
            "Epoch 75/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5504\n",
            "Epoch 76/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5213\n",
            "Epoch 77/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5212\n",
            "Epoch 78/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5211\n",
            "Epoch 79/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5212\n",
            "Epoch 80/100\n",
            "706/706 [==============================] - 92s 131ms/step - loss: 8.5210\n",
            "Epoch 81/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5212\n",
            "Epoch 82/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5247\n",
            "Epoch 83/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5215\n",
            "Epoch 84/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5209\n",
            "Epoch 85/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5246\n",
            "Epoch 86/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5219\n",
            "Epoch 87/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5247\n",
            "Epoch 88/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5246\n",
            "Epoch 89/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5289\n",
            "Epoch 90/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5228\n",
            "Epoch 91/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5229\n",
            "Epoch 92/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5227\n",
            "Epoch 93/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5229\n",
            "Epoch 94/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5227\n",
            "Epoch 95/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5231\n",
            "Epoch 96/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5233\n",
            "Epoch 97/100\n",
            "706/706 [==============================] - 92s 131ms/step - loss: 8.5232\n",
            "Epoch 98/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5258\n",
            "Epoch 99/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5242\n",
            "Epoch 100/100\n",
            "706/706 [==============================] - 92s 130ms/step - loss: 8.5260\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f90cc1471f0>"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(train_x, train_y,\n",
        "          batch_size=2048,\n",
        "          epochs=100,\n",
        "          callbacks=[model_checkpoint_callback]\n",
        "          )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(checkpoint_filepath)"
      ],
      "metadata": {
        "id": "6CrUO4VBENXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJi8dP_Su4gT"
      },
      "source": [
        "# Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQVhZMcuqdiZ"
      },
      "outputs": [],
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    if temperature <= 0:\n",
        "        return np.argmax(preds)\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "def generate_next(text, num_generated=10):\n",
        "    word_idxs = [word2idx(word) for word in text.lower().split()]\n",
        "    for i in range(num_generated):\n",
        "        # prediction = model.predict(x=np.array(word_idxs))\n",
        "        print(word_idxs)\n",
        "        x=np.array(word_idxs)\n",
        "        print(np.expand_dims(x, axis=0))\n",
        "        prediction = model.predict(np.expand_dims(x, axis=0))\n",
        "        idx = sample(prediction[-1], temperature=0.7)\n",
        "        word_idxs.append(idx)\n",
        "    return ' '.join(idx2word(idx) for idx in word_idxs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asL5rd-RvXwB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15f86078-d625-407b-80bc-e43948b5046c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 452]\n",
            "[[  0 452]]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "[0, 452, 10]\n",
            "[[  0 452  10]]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "[0, 452, 10, 116]\n",
            "[[  0 452  10 116]]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "[0, 452, 10, 116, 2403]\n",
            "[[   0  452   10  116 2403]]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "[0, 452, 10, 116, 2403, 42]\n",
            "[[   0  452   10  116 2403   42]]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "[0, 452, 10, 116, 2403, 42, 8]\n",
            "[[   0  452   10  116 2403   42    8]]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "[0, 452, 10, 116, 2403, 42, 8, 10]\n",
            "[[   0  452   10  116 2403   42    8   10]]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "[0, 452, 10, 116, 2403, 42, 8, 10, 8]\n",
            "[[   0  452   10  116 2403   42    8   10    8]]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "[0, 452, 10, 116, 2403, 42, 8, 10, 8, 22]\n",
            "[[   0  452   10  116 2403   42    8   10    8   22]]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "[0, 452, 10, 116, 2403, 42, 8, 10, 8, 22, 8670]\n",
            "[[   0  452   10  116 2403   42    8   10    8   22 8670]]\n",
            "1/1 [==============================] - 0s 26ms/step\n"
          ]
        }
      ],
      "source": [
        "given_word = \"The car\"\n",
        "generated_text = generate_next(given_word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9wve0n43JIH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f56f4026-ca65-4a02-f598-e226b616ee42"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the car that made empire up it that it said puzzling unnecessary'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "generated_text"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}